# -*- coding: utf-8 -*-
"""Malware analysis and detection using machine learning algorithms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CzeIHFPWt4tCdXjq7ze8QvetG-n0kt3-
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Function to upload a file from your local system
from google.colab import files
uploaded = files.upload()

# Load and prepare the data
import io
df = pd.read_csv(io.BytesIO(uploaded['trainLabels.csv']))
df['Class'] = df['Class'] - 1  # Adjust labels to start at 0

# Generate synthetic features for demonstration
np.random.seed(42)
df['Feature1'] = np.random.normal(size=len(df))
df['Feature2'] = np.random.normal(size=len(df))
df['Feature3'] = np.random.normal(size=len(df))

# Split the data
X = df[['Feature1', 'Feature2', 'Feature3']]
y = df['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Handling class imbalance with SMOTE
smote = SMOTE()
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Define the model
model = Sequential([
    Dense(64, activation='relu', input_dim=X_train_smote.shape[1]),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(9, activation='softmax')  # Assuming 9 classes
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_smote, y_train_smote, epochs=50, batch_size=32, validation_split=0.2, verbose=2)

# Evaluate the model
evaluation = model.evaluate(X_test, y_test, verbose=0)
print(f'Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}')

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Setting up the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 15, 20],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the Random Forest model
rf_model = RandomForestClassifier(random_state=42)

# Set up the GridSearch with cross-validation
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2)
grid_search.fit(X_train_smote, y_train_smote)

# Best model after grid search
best_rf_model = grid_search.best_estimator_

# Evaluation on validation set
val_predictions = best_rf_model.predict(X_val)
print("Validation Results:")
print(classification_report(y_val, val_predictions))

# Evaluation on the test set
test_predictions = best_rf_model.predict(X_test)
print("Test Results:")
print(classification_report(y_test, test_predictions))

from tensorflow.keras.callbacks import EarlyStopping

# Define model
model = Sequential([
    Dense(64, activation='relu', input_dim=X_train_smote.shape[1]),
    Dropout(0.6),  # Increased dropout
    Dense(64, activation='relu'),
    Dropout(0.6),  # Increased dropout
    Dense(9, activation='softmax')
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model with early stopping
history = model.fit(X_train_smote, y_train_smote, epochs=50, batch_size=32,
                    validation_split=0.2, verbose=2, callbacks=[early_stopping])

# Evaluate the model
evaluation = model.evaluate(X_test, y_test, verbose=0)
print(f'Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}')

from sklearn.utils.class_weight import compute_class_weight

# Calculate class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_train_smote), y=y_train_smote)
class_weight_dict = dict(enumerate(class_weights))

# Train the model using class weights
history = model.fit(X_train_smote, y_train_smote, epochs=50, batch_size=32,
                    validation_split=0.2, verbose=2, callbacks=[early_stopping],
                    class_weight=class_weight_dict)

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE

# Assuming 'X' and 'y' are your feature matrix and target array
# Split the data into training and remaining data, then split remaining into validation and test sets
X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.4, random_state=42)  # 60% for training
X_val, X_test, y_val, y_test = train_test_split(X_remaining, y_remaining, test_size=0.5, random_state=42)  # 20% each for validation and testing

# Handling class imbalance with SMOTE for the training data
smote = SMOTE()
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Prepare the Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)

# Fit the model on the SMOTE-resampled training data
dt_model.fit(X_train_smote, y_train_smote)

# Evaluate on the validation set
val_predictions = dt_model.predict(X_val)
print("Validation Results:")
print(classification_report(y_val, val_predictions))

# Evaluate on the test set
test_predictions = dt_model.predict(X_test)
print("Test Results:")
print(classification_report(y_test, test_predictions))